from pathlib import Path
import shutil
import tempfile
from typing import Awaitable

from loguru import logger
from pydantic import BaseModel

from domainslm.async_util import gather_with_semaphore
from domainslm.openai_util import (
    AgentRunFailure,
    AgentWrapper,
    AgentsSDKModel,
    SimpleMessageItem,
    SimpleReasoningItem,
    setup_openai_tracing,
)
from domainslm.vllm import VLLMSetup
from spider_eval.exec_eval import eval_exec_match, exec_on_db
from sql_agent_rewrite import SQLAgent
from train_sql_agent import RL_TRAINING_CONFIG
import polars as pl


class ReflectionInput(BaseModel):
    question: str
    behavior: list[SimpleReasoningItem | SimpleMessageItem]
    gt: str
    exec_result: str
    gt_exec_result: str


class ReflectionOutput(BaseModel):
    analysis: str
    refined_reasoning: str
    revised_query: str


class ReflectionResult(BaseModel):
    input: ReflectionInput
    output: ReflectionOutput

    def as_md(self) -> str:
        behavior_str = "\n".join(
            [f"- ({item.role}) {item.content}" for item in self.input.behavior]
        )
        md = f"""
# Reflection Input
## Question
{self.input.question}
## Agent's Behavior
{behavior_str}
## Ground Truth Query
{self.input.gt}
## Execution Result of Generated Query
{self.input.exec_result}
## Execution Result of Ground Truth Query
{self.input.gt_exec_result}
## Reflection Output
## Analysis
{self.output.analysis}
## Refined Reasoning
{self.output.refined_reasoning}
## Revised Query
{self.output.revised_query}
""".strip()
        return md


class ReflectionResults(BaseModel):
    results: list[ReflectionResult]


async def process_one_sample(
    original_db_path: Path,
    db_path: Path,
    question: str,
    ground_truth: str,
    model: AgentsSDKModel,
    truncation_limit: int,
) -> ReflectionResult | None:
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_dir = Path(temp_dir)
        db_path = temp_dir / original_db_path.name
        shutil.copyfile(original_db_path, db_path)
        print(f"Question: {question}")
        print(f"Ground Truth: {ground_truth}")
        agent = SQLAgent(
            db=f"sqlite:///{db_path}",
            model=model,
        )
        try:
            result = await agent.run_agent(question)
        except AgentRunFailure as e:
            logger.warning(f"Agent execution failed: {e.cause} - {str(e)}")
            return None

        exec_result = await exec_on_db(
            sqlite_path=db_path,
            query=result.final_output.query,
        )
        gt_exec_result = await exec_on_db(
            sqlite_path=db_path,
            query=ground_truth,
        )
        if eval_exec_match(
            db=str(db_path),
            p_str=result.final_output.query,
            g_str=ground_truth,
            plug_value=False,
            keep_distinct=False,
            progress_bar_for_each_datapoint=False,
        ):
            print("Execution Match: Yes")
            return None
    exec_result_str = (
        str(exec_result[1][:truncation_limit])
        if exec_result[0] == "result"
        else f"Error: {str(exec_result[1])}"
    )
    gt_exec_result_str = (
        str(gt_exec_result[1][:truncation_limit])
        if gt_exec_result[0] == "result"
        else f"Error: {str(gt_exec_result[1])}"
    )
    try:
        reflection = await generate_reflected_response(
            model=model,
            reflection_input=ReflectionInput(
                question=question,
                behavior=result.simplified(),
                gt=ground_truth,
                exec_result=exec_result_str,
                gt_exec_result=gt_exec_result_str,
            ),
        )
    except AgentRunFailure as e:
        logger.warning(f"Reflection generation failed: {e.cause} - {str(e)}")
        return None
    reflection_result = ReflectionResult(
        input=ReflectionInput(
            question=question,
            behavior=result.simplified(),
            gt=ground_truth,
            exec_result=exec_result_str,
            gt_exec_result=gt_exec_result_str,
        ),
        output=reflection,
    )
    return reflection_result


async def generate_reflected_response(
    model: AgentsSDKModel,
    reflection_input: ReflectionInput,
) -> ReflectionOutput:
    reflection_agent = AgentWrapper[ReflectionOutput].create(
        name="SQLReflectionAgent",
        instructions="""
You are an expert SQL agent that reflects on the quality of SQL queries generated by another agent.
You will see:
1. The original question.
2. The agent's reasoning process followed by the generated SQL query.
3. The ground truth SQL query.
4. SQL execution results for the generated query.
5. SQL execution results for the ground truth query.

Based on the above, provide:
- An analysis about agent's reasoning process.
- The refined reasoning steps the agent should have taken. Note that this has to be self-contained so that this can directly be used as its supervised training data.
- A revised SQL query that better matches the ground truth (This should highly likely to be the same as the ground truth).
""",
        model=model,
        output_type=ReflectionOutput,
    )
    input_str = f"""
Question: 
{reflection_input.question}

Agent's Reasoning and Query:
{"\n".join([item.content for item in reflection_input.behavior])}

Ground Truth Query: {reflection_input.gt}

Execution Results of Generated Query: {reflection_input.exec_result}

Execution Results of Ground Truth Query: {reflection_input.gt_exec_result}
""".strip()
    ret = await reflection_agent.run(input=input_str)

    return ret.final_output()


async def main():
    truncation_limit = 1500
    setup_openai_tracing()
    vllm_setup = VLLMSetup.qwen3()
    await vllm_setup.ensure_vllm_running()
    db_root = Path("data/database")
    train_data = (
        pl.read_parquet(RL_TRAINING_CONFIG["data"]["train_files"])
        .head(5)
        .iter_rows(named=True)
    )

    tasks: list[Awaitable[ReflectionResult | None]] = []
    for row in train_data:
        db_id: str = row["db_id"]
        question: str = row["question"]
        ground_truth: str = row["query"]
        original_db_path = db_root / db_id / f"{db_id}.sqlite"
        db_path = original_db_path  # Using original DB directly
        tasks.append(
            process_one_sample(
                original_db_path=original_db_path,
                db_path=db_path,
                question=question,
                ground_truth=ground_truth,
                model=vllm_setup,
                truncation_limit=truncation_limit,
            )
        )

    reflection_results = await gather_with_semaphore(tasks, 10, progressbar=True)

    all_results = ReflectionResults(
        results=[result for result in reflection_results if result is not None]
    )
    out_path = Path("reflection_results.json")
    out_path.write_text(all_results.model_dump_json(indent=2))


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
